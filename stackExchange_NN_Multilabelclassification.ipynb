{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPlWmDXu/ZvqC1ozQCATjGT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/parimalakettymuthu/MachineLearning-Projects/blob/main/stackExchange_NN_Multilabelclassification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "yWiXXVeW_cfv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "outputId": "412316d2-43f9-4258-f606-d3ebcf63cf44"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-1ff5bc90a92e>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'load_ext'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'autoreload    #Added code for autoreload'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'autoreload'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_line_magic\u001b[0;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[1;32m   2416\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'local_ns'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_local_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2417\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2418\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2419\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<decorator-gen-57>\u001b[0m in \u001b[0;36mload_ext\u001b[0;34m(self, module_str)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/IPython/core/magics/extension.py\u001b[0m in \u001b[0;36mload_ext\u001b[0;34m(self, module_str)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmodule_str\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mUsageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Missing module name.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextension_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_extension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'already loaded'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/IPython/core/extensions.py\u001b[0m in \u001b[0;36mload_extension\u001b[0;34m(self, module_str)\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmodule_str\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mprepended_to_syspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mipython_extension_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m                     \u001b[0mmod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mmod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mipython_extension_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m                         print((\"Loading extensions from {dir} is deprecated. \"\n",
            "\u001b[0;32m/usr/lib/python3.9/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.9/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.9/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.9/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'autoreload    #Added code for autoreload'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "#Added code for autoreload\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys    #Access the google drive, basepath, install required packages and setted path for custom-functions\n",
        "if 'google.colab' in str(get_ipython()):\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "\n",
        "  !pip install torchtext --upgrade --q\n",
        "  !pip install torchmetrics --q\n",
        "  !pip install -quiet torch-lr-finder --q\n",
        "  !pip install wandb --q --upgrade\n",
        "\n",
        "  basepath = '/content/drive/My Drive/NLP' \n",
        "  sys.path.append('/content/drive/My Drive/NLP/custom-functions')"
      ],
      "metadata": {
        "id": "roTXjiPGC3ax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing the required packages \n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torchmetrics\n",
        "from torchmetrics.classification import MultilabelF1Score, MultilabelHammingDistance\n",
        "from torchmetrics.functional.classification import multilabel_f1_score, multilabel_hamming_distance\n",
        "\n",
        "import joblib\n",
        "import ast\n",
        "import wandb\n",
        "\n",
        "from types import SimpleNamespace\n",
        "from functools import partial\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MultiLabelBinarizer as mlb\n"
      ],
      "metadata": {
        "id": "byKpUJjmDgYh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Defined the google drive folders for accessing/saving models related files\n",
        "embeddings_folder = Path(basepath)/ 'assignment7/WordEmbeddings'\n",
        "data_folder = Path(basepath)/ 'assignment7/MultiLabel_Classification'\n",
        "model_saving_folder = Path(basepath)/ 'assignment7/MultiLabel_Classification'"
      ],
      "metadata": {
        "id": "GmNdHyTvDhwb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_cleaned = data_folder/ \"df_multilabel_hw_cleaned.joblib\"   #Access the joblib file and load using the command\n",
        "stackExchange_dataset = joblib.load(data_cleaned)"
      ],
      "metadata": {
        "id": "-xOnlKc1Ydmo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = stackExchange_dataset['cleaned_text'].values  #Access the X & y value from the imported dataset\n",
        "y = stackExchange_dataset['Tag_Number'].values"
      ],
      "metadata": {
        "id": "xbf44Q-XYrvw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install swifter -qq  #Installed the swifter package"
      ],
      "metadata": {
        "id": "89QcmYtfYyWo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import swifter   #Imported the required libraries and convert the Tag_Number field using the given function\n",
        "import ast\n",
        "stackExchange_dataset['Tag_Number_list'] = stackExchange_dataset['Tag_Number'].swifter.apply(lambda x: ast.literal_eval(x))"
      ],
      "metadata": {
        "id": "5b-mizHzY21P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_final = stackExchange_dataset['Tag_Number_list'].values  "
      ],
      "metadata": {
        "id": "jOP5vGOIZwiZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MultiLabelBinarizer as mlb    #Imported module for creating multi-label-binaries\n",
        "y_stackExchange_encoding = mlb().fit_transform(y_final)"
      ],
      "metadata": {
        "id": "oQX_RJsUZtz6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split   #Performed the datasplit and created train, test & valid dataset\n",
        "X_sExchange_train, X_valid_test, y_sExchange_train, y_valid_test = train_test_split(X, y_stackExchange_encoding, test_size=0.4, random_state=42)\n",
        "X_sExchange_valid, X_sExchange_test, y_sExchange_valid, y_sExchange_test = train_test_split(X_valid_test, y_valid_test, test_size=0.5, random_state=42)"
      ],
      "metadata": {
        "id": "awTdtZf6Y6yI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import KeyedVectors #\n",
        "pretrained_sExchange_file = str(embeddings_folder/ \"model_stackExchange_CBOW.bin\")\n",
        "sExchange_vectors = KeyedVectors.load(pretrained_sExchange_file)"
      ],
      "metadata": {
        "id": "zpz6KhcoZsIJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch_lr_finder -qq  #Installed the torch lr finder module"
      ],
      "metadata": {
        "id": "5FAncfudb53H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_lr_finder import LRFinder  #Imported required custom & other module\n",
        "from Trainer_v4 import Trainer\n",
        "from data_preparation_HW7 import * "
      ],
      "metadata": {
        "id": "ttiIxeiaaCA6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from ff_sequential_model import MLPCustom\n",
        "from ff_sequential_model_v1 import MLPCustom"
      ],
      "metadata": {
        "id": "tuj-uxyKbcM4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating training dataset with subset \n",
        "import random\n",
        "Trainer.set_seed()\n",
        "sExchange_trainset = CustomDataset(X_sExchange_train, y_sExchange_train)\n",
        "se_train_subset_indices = random.sample(range(0, len(sExchange_trainset)), 1000)\n",
        "se_train_subset = torch.utils.data.Subset(sExchange_trainset, se_train_subset_indices)\n",
        "se_vocab = get_vocab(sExchange_trainset, min_freq=2)"
      ],
      "metadata": {
        "id": "AdzD3XA-d3Hk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(se_vocab), se_vocab['debug']    #Displayed the type of vocab generated and index of string \"debug\""
      ],
      "metadata": {
        "id": "cbbffFS0wqhh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "bWWMlSLUhiW2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pretrained_sExchange_file = str(embeddings_folder/ \"model_stackExchange_CBOW.bin\")  #Imported the pre-traine vectors\n",
        "sExchange_vectors = KeyedVectors.load(pretrained_sExchange_file)"
      ],
      "metadata": {
        "id": "8wGZYxg7eLuu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pretrained_weights, words_found, words_not_found = get_pretrained_weights(\n",
        "    vocab = se_vocab,\n",
        "    pretrained_vectors = sExchange_vectors,\n",
        "    embedding_dim = 300, #updated from 300 to 150\n",
        ")"
      ],
      "metadata": {
        "id": "Lq8qPuGLlOUh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialized the required HyperParameters "
      ],
      "metadata": {
        "id": "2cxT_0v8fgot"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hyperparameters = SimpleNamespace(\n",
        "# for model\n",
        "    EMBED_DIM=300,\n",
        "    VOCAB_SIZE=len(se_vocab),\n",
        "    OUTPUT_DIM=10,\n",
        "    HIDDEN_SIZES_LIST=[],\n",
        "    DPROB_LIST=[],\n",
        "    NON_LINEARITY=nn.SELU(),\n",
        "    BATCH_NORM=False,\n",
        "    \n",
        "    # for optimizer\n",
        "    OPTIMIZER=\"AdamW\",\n",
        "    MOMENTUM=0,\n",
        "    NESTEROV=False,\n",
        "    \n",
        "    # for training\n",
        "    INITIALIZATION=\"default\",\n",
        "    EPOCHS=20,\n",
        "    BATCH_SIZE=100,\n",
        "    LEARNING_RATE=0.001,\n",
        "    DATASET=\"STACKEXCHANGE\",\n",
        "    ARCHITECTURE=\"emdbag_linear\",\n",
        "    \n",
        "    # Schedulers\n",
        "    SCHEDULER=\"None\",\n",
        "\n",
        "    #gradient clipping\n",
        "    CLIP_TYPE='norm',\n",
        "    CLIP_VALUE=2\n",
        ")"
      ],
      "metadata": {
        "id": "1aocgXe7vws4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Specifying run name & folder"
      ],
      "metadata": {
        "id": "iR3EXC18iovu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Defined the path for saving model and run name\n",
        "project_name = \"StackExchange NN architecture\"\n",
        "run_name=\"Task3a experiment18 version11\"\n",
        "run_folder = model_saving_folder / run_name\n",
        "run_folder.mkdir(exist_ok=True)\n",
        "log_frequency = 5"
      ],
      "metadata": {
        "id": "VPyitAyTilNb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modifying Hyperparameters for Run"
      ],
      "metadata": {
        "id": "iEAjOIYKjGtD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "########EXECUTED SEVERAL EXPERIMENTS USING DIFFERENT COMBINATIONS ######\n",
        "\n",
        "# # run 1 - based on default initialization     \n",
        "# # Notes: Final Learning rate was set to 1\n",
        "\n",
        "# # run 2\n",
        "hyperparameters.HIDDEN_SIZES_LIST = [200]\n",
        "hyperparameters.DPROB_LIST = [0]\n",
        "hyperparameters.LEARNING_RATE = 0.001  # reset initial learning rate\n",
        "# # Notes : Final Learning rate was set to 1.18\n",
        "\n",
        "# # run 3\n",
        "#hyperparameters.OPTIMIZER = \"Adam\"\n",
        "hyperparameters.LEARNING_RATE = 0.001  # reset initial learning rate\n",
        "# # Notes : Final Learning rate was set to 0.006\n",
        "\n",
        "# # run 4\n",
        "hyperparameters.HIDDEN_SIZES_LIST = [200] + [200]\n",
        "hyperparameters.DPROB_LIST = [0] + [0]\n",
        "hyperparameters.LEARNING_RATE = 0.001  # reset initial learning rate\n",
        "# # Notes : Final Learning rate was set to 0.003\n",
        "\n",
        "# # run 5\n",
        "hyperparameters.INITIALIZATION = 'kaiming'\n",
        "hyperparameters.NON_LINEARITY = nn.ReLU()\n",
        "hyperparameters.LEARNING_RATE = 0.001 \n",
        "# # Notes : Final Learning rate was set to 0.003\n",
        "\n",
        "# # run 6\n",
        "hyperparameters.HIDDEN_SIZES_LIST = [400] \n",
        "hyperparameters.DPROB_LIST = [0] \n",
        "hyperparameters.LEARNING_RATE = 0.001  \n",
        "# # Notes : Final Learning rate was set to 0.005\n",
        "\n",
        "##run 7\n",
        "hyperparameters.HIDDEN_SIZES_LIST = [100] + [200] +[200]\n",
        "hyperparameters.DPROB_LIST = [0] + [0] +[0]\n",
        "hyperparameters.LEARNING_RATE = 0.001  \n",
        "# # Notes : Final Learning rate was set to 0.016\n",
        "\n",
        "#run 8 \n",
        "hyperparameters.HIDDEN_SIZES_LIST = [400] + [300] +[200] \n",
        "hyperparameters.DPROB_LIST = [0] + [0] +[0]\n",
        "hyperparameters.LEARNING_RATE = 0.001\n",
        "# Notes : Final Learning rate was set to 1\n",
        "\n",
        "#run9\n",
        "hyperparameters.HIDDEN_SIZES_LIST = [400] + [300] +[200]\n",
        "hyperparameters.DPROB_LIST = [0.5] + [0.5] +[0.5]\n",
        "hyperparameters.LEARNING_RATE = 0.001\n",
        "# Notes : Final Learning rate was set to 2\n",
        "\n",
        "#run10\n",
        "hyperparameters.HIDDEN_SIZES_LIST = [500] + [400] +[300]\n",
        "hyperparameters.DPROB_LIST = [0] + [0] +[0]\n",
        "hyperparameters.LEARNING_RATE = 0.001\n",
        "# Notes : Final Learning rate was set to 0.9 \n",
        "\n",
        "#run 11\n",
        "hyperparameters.HIDDEN_SIZES_LIST = [600] + [500] +[400]\n",
        "hyperparameters.DPROB_LIST = [0] + [0] +[0]\n",
        "hyperparameters.LEARNING_RATE = 0.001\n",
        "# Notes : Final Learning rate was set to 0.1\n",
        "\n",
        "#run 12\n",
        "hyperparameters.HIDDEN_SIZES_LIST = [600] + [600] +[400]\n",
        "hyperparameters.DPROB_LIST = [0] + [0] +[0]\n",
        "hyperparameters.LEARNING_RATE = 0.001\n",
        "# Notes : Final Learning rate was set to 0.5 \n",
        "\n",
        "#run13\n",
        "hyperparameters.HIDDEN_SIZES_LIST = [500] + [400] +[300]\n",
        "hyperparameters.DPROB_LIST = [0] + [0] +[0]\n",
        "hyperparameters.LEARNING_RATE = 0.001\n",
        "# Notes : Final Learning rate was set to 0.1\n",
        "\n",
        "#run14\n",
        "hyperparameters.HIDDEN_SIZES_LIST = [600] + [500] +[400]\n",
        "hyperparameters.DPROB_LIST = [0.2] + [0.1] +[0.1]\n",
        "hyperparameters.LEARNING_RATE = 0.001\n",
        "# Notes : Final Learning rate was set to 0.1\n",
        "\n",
        "#run14\n",
        "hyperparameters.HIDDEN_SIZES_LIST = [600] + [500] +[400]\n",
        "hyperparameters.DPROB_LIST = [0.2] + [0.2] +[0.2]\n",
        "hyperparameters.LEARNING_RATE = 0.001\n",
        "# # Notes : Final Learning rate was set to 0.1\n",
        "\n",
        "#run15 \n",
        "hyperparameters.DPROB_LIST = [0.2] + [0.2] +[0.1]\n",
        "CLIP_TYPE='norm',\n",
        "CLIP_VALUE=2,\n",
        "EPOCHS=10,\n",
        "hyperparameters.WEIGHT_DECAY = 0.1\n",
        "# # Notes : Final Learning rate was set to 0.1\n",
        "\n",
        "#run 16\n",
        "#One cycle scheduler\n",
        "hyperparameters.LEARNING_RATE = 0.001   \n",
        "hyperparameters.WEIGHT_DECAY = 10\n",
        "# hyperparameters.SCHEDULER='OneCyclicLR'\n",
        "# hyperparameters.SCHEDULER_MAX_LR=0.01\n",
        "# hyperparameters.SCHEDULER_DIV_FACTOR=25\n",
        "# hyperparameters.SCHEDULER_FINAL_DIV_FACTOR=1e3\n",
        "hyperparameters.EPOCHS = 10\n",
        "# # Notes : Final Learning rate was set to 0.1\n",
        "\n",
        "#run17\n",
        "hyperparameters.LEARNING_RATE = 0.001 \n",
        "hyperparameters.HIDDEN_SIZES_LIST = [200]+ [200] \n",
        "CLIP_VALUE=10\n",
        "hyperparameters.WEIGHT_DECAY = 5\n",
        "# # Notes : Final Learning rate was set to 0.5\n",
        "\n",
        "#run18\n",
        "hyperparameters.LEARNING_RATE = 0.001 \n",
        "hyperparameters.HIDDEN_SIZES_LIST = [100]+ [200]+[200]\n",
        "#hyperparameters.DPROB_LIST = [0] + [0] +[0]\n",
        "CLIP_VALUE=2\n",
        "hyperparameters.EPOCHS = 20\n",
        "hyperparameters.WEIGHT_DECAY = 0\n",
        "# # Notes : Final Learning rate was set to 0.1"
      ],
      "metadata": {
        "id": "Mz6HeXcUusz1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Configuring the trainer "
      ],
      "metadata": {
        "id": "5d24Bq1lk3MB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Invoked the set_seed from Trainer class and created the partial function using generated vocabulary\n",
        "Trainer.set_seed()\n",
        "se_collate_fn = partial(collate_batch, vocab=se_vocab)"
      ],
      "metadata": {
        "id": "qXtU7DtFk2ny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Created Data Loader, initialized the loss function, model, optimizer & device\n",
        "se_train_loader, _ = get_loaders(trainset=sExchange_trainset, validset=None, \n",
        "                                         batch_size_=hyperparameters.BATCH_SIZE,\n",
        "                                         collate_fn=se_collate_fn)\n",
        "\n",
        "se_loss_function = nn.BCEWithLogitsLoss()\n",
        "stackExchange_model = MLPCustom(hyperparameters.EMBED_DIM,\n",
        "                                hyperparameters.VOCAB_SIZE,\n",
        "                                hyperparameters.HIDDEN_SIZES_LIST,\n",
        "                                hyperparameters.DPROB_LIST, \n",
        "                                hyperparameters.OUTPUT_DIM, \n",
        "                                hyperparameters.NON_LINEARITY,\n",
        "                                hyperparameters.BATCH_NORM,))\n",
        "def init_weights(m):\n",
        "  if type(m)==\"nn.Linear\":\n",
        "    torch.nn.init.kaiming_normal_(m.weight)\n",
        "    torch.nn.init.zeros_(nn.bias)\n",
        "\n",
        "#Apply initialization to all modules\n",
        "if hyperparameters.INITIALIZATION == 'kaiming':\n",
        "  stackExchange_model.apply(init_weights)\n",
        "\n",
        "\n",
        "#Defining optimizer\n",
        "def get_optimizer():\n",
        "  if hyperparameters.OPTIMIZER == 'SGD':\n",
        "    optimizer = torch.optim.SGD(\n",
        "        stackExchange_model.parameters(),\n",
        "        lr = hyperparameters.LEARNING_RATE,\n",
        "        momentum = hyperparameters.MOMENTUM,\n",
        "        nesterov = hyperparameters.NESTEROV,\n",
        "        weight_decay = hyperparameters.WEIGHT_DECAY\n",
        "    )\n",
        "  else:\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        stackExchange_model.parameters(),\n",
        "        lr = hyperparameters.LEARNING_RATE,\n",
        "        weight_decay = hyperparameters.WEIGHT_DECAY\n",
        "    )\n",
        "  return optimizer\n",
        "\n",
        "sExchange_optimizer = get_optimizer()\n",
        "sExchange_device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "P0JRv-hjNb77"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sExchange_optimizer   #Displayed the initialized optimizer"
      ],
      "metadata": {
        "id": "WyEhsfq7OF79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stackExchange_model   #Displayed the initialized model"
      ],
      "metadata": {
        "id": "efkzX1wUdGK0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trainer"
      ],
      "metadata": {
        "id": "bas5zQrwdPmj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Created the Trainer object & invoked several set functions\n",
        "sEXchange_trainer = Trainer(\n",
        "    model=stackExchange_model, optimizer=sExchange_optimizer, \n",
        "    criterion=se_loss_function,\n",
        "    device=sExchange_device\n",
        ")\n",
        "sEXchange_trainer.set_optimizer(get_optimizer())\n",
        "sEXchange_trainer.set_loaders(se_train_loader)\n",
        "sEXchange_trainer.set_gradient_clipping(hyperparameters.CLIP_TYPE, hyperparameters.CLIP_VALUE, norm_type=2)\n",
        "steps_per_epoch = len(sEXchange_trainer.train_loader)\n",
        "# scheduler = torch.optim.lr_scheduler.OneCycleLR(sEXchange_trainer.optimizer, \n",
        "#                                                 hyperparameters.SCHEDULER_MAX_LR, \n",
        "#                                                 steps_per_epoch=steps_per_epoch,\n",
        "#                                                 epochs=hyperparameters.EPOCHS,\n",
        "#                                                 div_factor=hyperparameters.SCHEDULER_DIV_FACTOR,\n",
        "#                                                 final_div_factor=hyperparameters.SCHEDULER_FINAL_DIV_FACTOR)\n",
        "# sEXchange_trainer.set_lr_scheduler(scheduler=scheduler)"
      ],
      "metadata": {
        "id": "8EzS-2JDdJRa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Learning Rate Finder"
      ],
      "metadata": {
        "id": "4gE7wNq_emrR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Called the learning rate finder function to suggest LR based on the hyperparameters defined on the model\n",
        "sEXchange_trainer.lr_finder_range_test(se_train_loader)"
      ],
      "metadata": {
        "id": "LZsdhPy-el6q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hyperparameters.LEARNING_RATE = 0.1   #Based on suggestion, initialized learning rate"
      ],
      "metadata": {
        "id": "mwdjeuJviJx1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set trainer based on hyperparameters"
      ],
      "metadata": {
        "id": "eDXINAJ9i-nC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Resetting optimizer\n",
        "sEXchange_trainer.set_optimizer(get_optimizer())\n",
        "\n",
        "#Setting metric for multilabel classification\n",
        "se_train_metric = MultilabelHammingDistance(num_labels=10)\n",
        "sEXchange_trainer.set_metric(se_train_metric.to(sExchange_device))"
      ],
      "metadata": {
        "id": "YwmeeecQi9nj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Setting wandb \n",
        "sEXchange_trainer.set_wandb(\n",
        "   project_name = project_name,\n",
        "   run_name = run_name,\n",
        "   config = hyperparameters,\n",
        "   log_batch=True, \n",
        "   log_frequency=log_frequency\n",
        " )"
      ],
      "metadata": {
        "id": "lzR_TCxXkWRe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sEXchange_trainer.learning_rates"
      ],
      "metadata": {
        "id": "9glKMW3wkuk2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sanity Check"
      ],
      "metadata": {
        "id": "xzFz0pXBxmJc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Performing the sanity check\n",
        "sEXchange_trainer.sanity_check(num_classes=2)   "
      ],
      "metadata": {
        "id": "Pb0sAh6akxu-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Training the model\n",
        "sEXchange_trainer.train(num_epochs=hyperparameters.EPOCHS, multilabel=True)"
      ],
      "metadata": {
        "id": "HQvp2LoLxszY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plotting the graph based on model training\n",
        "sEXchange_trainer.plot_history()"
      ],
      "metadata": {
        "id": "UQ1_Lz9uyaQB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finish Run"
      ],
      "metadata": {
        "id": "Sax6tSQF38Tf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.finish()  #Displaying the cumulative results using wandb function"
      ],
      "metadata": {
        "id": "7VknFTpY30vr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GYKF9dP93-oq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}