{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNPpmdB7Ckima3Db4gHUSE5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/parimalakettymuthu/MachineLearning-Projects/blob/main/stackExchange_Final_NN_Multilabel_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "lD103gpJGRUv"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "if 'google.colab' in str(get_ipython()):\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "\n",
        "  !pip install torchtext --upgrade --q\n",
        "  !pip install torchmetrics --q\n",
        "  !pip install --quiet torch-lr-finder --q\n",
        "  !pip install --upgrade wandb --q\n",
        "  !pip install --gensim --q\n",
        "\n",
        "  basepath = '/content/drive/My Drive/NLP'\n",
        "  sys.path.append('/content/drive/My Drive/NLP/custom-functions')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "niYpbqdmGee8",
        "outputId": "e1fd9a4a-d344-4189-b5e9-ddb20b792da1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.2/519.2 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m198.8/198.8 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\n",
            "Usage:   \n",
            "  pip3 install [options] <requirement specifier> [package-index-options] ...\n",
            "  pip3 install [options] -r <requirements file> [package-index-options] ...\n",
            "  pip3 install [options] [-e] <vcs project url> ...\n",
            "  pip3 install [options] [-e] <local project path> ...\n",
            "  pip3 install [options] <archive url/path> ...\n",
            "\n",
            "no such option: --gensim\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing the required libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torchmetrics\n",
        "from torchmetrics.classification import MultilabelF1Score, MulticlassHammingDistance\n",
        "from torchmetrics.functional.classification import multilabel_f1_score,multilabel_hamming_distance\n",
        "\n",
        "import joblib\n",
        "import ast\n",
        "import wandb\n",
        "\n",
        "from types import SimpleNamespace\n",
        "from functools import partial\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MultiLabelBinarizer as mlb"
      ],
      "metadata": {
        "id": "PnlDottZHGp2"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Defined the google drive folders for accessing/saving model progress\n",
        "embeddings_folder = Path(basepath)/ 'assignment7/WordEmbeddings'\n",
        "data_folder = Path(basepath)/ 'assignment7/MultiLabel_Classification'\n",
        "model_saving_folder = Path(basepath)/ 'assignment7/NN_MultiLabel_Classification_task3b'\n"
      ],
      "metadata": {
        "id": "e-j3MGnXIv_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_cleaned = data_folder/ \"df_multilabel_hw_cleaned.joblib\"\n",
        "stackExchange_dataset = joblib.load(data_cleaned)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "id": "ZUXNMBESJlBz",
        "outputId": "be8e9e91-085a-41d0-d3f6-d9e90765abfc"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-05f9259d7ce6>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata_cleaned\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_folder\u001b[0m\u001b[0;34m/\u001b[0m \u001b[0;34m\"df_multilabel_hw_cleaned.joblib\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mstackExchange_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_cleaned\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'data_folder' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = stackExchange_dataset['cleaned_text'].values\n",
        "y = stackExchange_dataset['Tag_Number'].values"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "id": "0e72yv8uJweQ",
        "outputId": "ae9d2e6b-8abb-41f3-c9b2-e8da73e76270"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-cfa6141dcdd5>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstackExchange_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cleaned_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstackExchange_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Tag_Number'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'stackExchange_dataset' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install swifter -qq"
      ],
      "metadata": {
        "id": "CKvLRqjYJ9YP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import swifter\n",
        "import ast\n",
        "stackExchange_dataset['Tag_number_list'] = stackExchange_dataset['Tag_Number'].swifter.apply(lambda x: ast.literal(x))"
      ],
      "metadata": {
        "id": "2Sp6rZbSKBUp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_final = stackExchange_dataset['Tag_Number_list'].values"
      ],
      "metadata": {
        "id": "NMAV6a5aKU6q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from sklearn.preprocessing import MultiLabelBinarizer as mlb\n",
        "y_stackExchange_encoding = mlb().fit_transform(y_final)"
      ],
      "metadata": {
        "id": "w_LBf1tDKX3v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_sExchange_train, X_valid_test, y_sExchange_train, y_valid_test = train_test_split(X, y_stackExchange_encoding, test_size=0.4, random_state=42)\n",
        "X_sExchange_valid, X_sExchange_test, y_sExchange_valid, y_sExchange_test = train_test_split(X_valid_test, y_valid_test, test_size=0.5, random_state=42)"
      ],
      "metadata": {
        "id": "3WQRfaoHKaej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import KeyedVectors\n",
        "pretrained_sExchange_file = str(embeddings_folder/ \"model_stackExchange_CBOW.bin\")\n",
        "sExchange_vectors = KeyedVectors.load(pretrained_sExchange_file)"
      ],
      "metadata": {
        "id": "5sz0-6-cKcuW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch_lr_finder -qq"
      ],
      "metadata": {
        "id": "msRp53LRKkuP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_lr_finder import LRFinder\n",
        "from Trainer_v4 import Trainer\n",
        "from data_preparation_HW7 import *"
      ],
      "metadata": {
        "id": "zqR58BTUKo3R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ff_sequential_model import MLPCustom"
      ],
      "metadata": {
        "id": "mfuTxEy8Kwvi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Creating Dataset & Vocab"
      ],
      "metadata": {
        "id": "k_fe2Z8SK8CX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "se_trainset = CustomDataset(X_sExchange_train, y_sExchange_train)\n",
        "se_validset = CustomDataset(X_sExchange_valid, y_sExchange_valid)\n",
        "se_testset = CustomDataset(X_sExchange_test, y_sExchange_test)\n",
        "\n",
        "#Creating stackexchange vocab\n",
        "stackExchange_vocab = get_vocab()"
      ],
      "metadata": {
        "id": "Peh49J0jK1AX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_preparation_HW7?"
      ],
      "metadata": {
        "id": "ixqPCRHJLqRZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}